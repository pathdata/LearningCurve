{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Data Exploration and Principal Component Analysis Tutorial\n",
    "\n",
    "**Author:** Dr. Priya Lakshmi Narayanan  \n",
    "**Institute:** Institute of Cancer Research\n",
    "\n",
    "This tutorial demonstrates practical approaches to working with healthcare data, handling missing data, detecting artifacts, and applying PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Electronic Health Record (EHR) Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ehr_data(n_patients=1000, n_features=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic EHR data with various feature types\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_patients : int\n",
    "        Number of patient records\n",
    "    n_features : int\n",
    "        Number of clinical features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        Synthetic EHR data\n",
    "    \"\"\"\n",
    "    # Patient demographics\n",
    "    age = np.random.normal(55, 15, n_patients).clip(18, 95)\n",
    "    gender = np.random.choice(['M', 'F'], n_patients)\n",
    "    \n",
    "    # Vital signs with realistic distributions\n",
    "    systolic_bp = np.random.normal(130, 20, n_patients).clip(90, 200)\n",
    "    diastolic_bp = np.random.normal(80, 12, n_patients).clip(60, 120)\n",
    "    heart_rate = np.random.normal(75, 12, n_patients).clip(50, 120)\n",
    "    temperature = np.random.normal(36.8, 0.5, n_patients).clip(35.5, 39.5)\n",
    "    \n",
    "    # Laboratory values\n",
    "    glucose = np.random.gamma(2, 50, n_patients).clip(60, 400)\n",
    "    hemoglobin = np.random.normal(14, 2, n_patients).clip(8, 18)\n",
    "    wbc = np.random.normal(7.5, 2.5, n_patients).clip(3, 15)\n",
    "    platelets = np.random.normal(250, 60, n_patients).clip(100, 450)\n",
    "    creatinine = np.random.gamma(2, 0.5, n_patients).clip(0.5, 3.0)\n",
    "    \n",
    "    # Biomarkers\n",
    "    cholesterol = np.random.normal(200, 40, n_patients).clip(120, 350)\n",
    "    hdl = np.random.normal(50, 15, n_patients).clip(20, 100)\n",
    "    ldl = np.random.normal(130, 35, n_patients).clip(50, 250)\n",
    "    triglycerides = np.random.gamma(2, 75, n_patients).clip(40, 400)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'patient_id': range(1, n_patients + 1),\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'systolic_bp': systolic_bp,\n",
    "        'diastolic_bp': diastolic_bp,\n",
    "        'heart_rate': heart_rate,\n",
    "        'temperature': temperature,\n",
    "        'glucose': glucose,\n",
    "        'hemoglobin': hemoglobin,\n",
    "        'wbc_count': wbc,\n",
    "        'platelet_count': platelets,\n",
    "        'creatinine': creatinine,\n",
    "        'total_cholesterol': cholesterol,\n",
    "        'hdl_cholesterol': hdl,\n",
    "        'ldl_cholesterol': ldl,\n",
    "        'triglycerides': triglycerides,\n",
    "    }\n",
    "    \n",
    "    # Add additional random features to reach n_features\n",
    "    for i in range(16, n_features):\n",
    "        data[f'feature_{i}'] = np.random.randn(n_patients)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some correlations for realism\n",
    "    bmi = np.random.normal(26, 5, n_patients).clip(15, 45)\n",
    "    df['bmi'] = bmi\n",
    "    df['systolic_bp'] += bmi * 0.5\n",
    "    df['glucose'] += bmi * 2\n",
    "    df['systolic_bp'] += age * 0.3\n",
    "    df['creatinine'] += (age - 40) * 0.01\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "ehr_data = generate_ehr_data(n_patients=500, n_features=30)\n",
    "\n",
    "print(f\"Generated EHR data shape: {ehr_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(ehr_data.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(ehr_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Distribution of Key Clinical Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "clinical_vars = ['age', 'systolic_bp', 'glucose', 'hemoglobin', 'creatinine', 'bmi']\n",
    "\n",
    "for idx, var in enumerate(clinical_vars):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(ehr_data[var], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax.set_title(f'{var.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Data: Introduction and Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_missing_data(df, missing_patterns):\n",
    "    \"\"\"\n",
    "    Introduce missing data with different mechanisms\n",
    "    \"\"\"\n",
    "    df_missing = df.copy()\n",
    "    \n",
    "    for pattern in missing_patterns:\n",
    "        feature = pattern['feature']\n",
    "        mechanism = pattern['mechanism']\n",
    "        rate = pattern['rate']\n",
    "        \n",
    "        n = len(df_missing)\n",
    "        \n",
    "        if mechanism == 'MCAR':\n",
    "            missing_mask = np.random.rand(n) < rate\n",
    "            df_missing.loc[missing_mask, feature] = np.nan\n",
    "            \n",
    "        elif mechanism == 'MAR':\n",
    "            condition_var = pattern.get('condition_var', 'age')\n",
    "            threshold = df_missing[condition_var].median()\n",
    "            prob = np.where(df_missing[condition_var] > threshold, rate * 2, rate * 0.5)\n",
    "            missing_mask = np.random.rand(n) < prob\n",
    "            df_missing.loc[missing_mask, feature] = np.nan\n",
    "            \n",
    "        elif mechanism == 'MNAR':\n",
    "            threshold = df_missing[feature].quantile(0.75)\n",
    "            prob = np.where(df_missing[feature] > threshold, rate * 3, rate * 0.3)\n",
    "            missing_mask = np.random.rand(n) < prob\n",
    "            df_missing.loc[missing_mask, feature] = np.nan\n",
    "    \n",
    "    return df_missing\n",
    "\n",
    "# Define missing data patterns\n",
    "missing_patterns = [\n",
    "    {'feature': 'glucose', 'mechanism': 'MCAR', 'rate': 0.10},\n",
    "    {'feature': 'hemoglobin', 'mechanism': 'MAR', 'rate': 0.15, 'condition_var': 'age'},\n",
    "    {'feature': 'creatinine', 'mechanism': 'MNAR', 'rate': 0.12},\n",
    "    {'feature': 'ldl_cholesterol', 'mechanism': 'MCAR', 'rate': 0.08},\n",
    "    {'feature': 'bmi', 'mechanism': 'MAR', 'rate': 0.10, 'condition_var': 'age'},\n",
    "]\n",
    "\n",
    "ehr_missing = introduce_missing_data(ehr_data, missing_patterns)\n",
    "\n",
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_matrix = ehr_missing.isnull()\n",
    "plt.imshow(missing_matrix.iloc[:100, :20].T, cmap='RdYlGn_r', aspect='auto', interpolation='none')\n",
    "plt.colorbar(label='Missing (1) vs Present (0)')\n",
    "plt.xlabel('Patient Index')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Missing Data Pattern Visualization', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Missing data summary\n",
    "print(\"Missing Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Feature': ehr_missing.columns,\n",
    "    'Missing_Count': ehr_missing.isnull().sum(),\n",
    "    'Missing_Percentage': (ehr_missing.isnull().sum() / len(ehr_missing) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "display(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Missing Data: Multiple Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare numeric data for imputation\n",
    "numeric_cols = ehr_missing.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('patient_id')\n",
    "X_missing = ehr_missing[numeric_cols].copy()\n",
    "\n",
    "print(f\"Working with {len(numeric_cols)} numeric features\")\n",
    "print(f\"Total missing values: {X_missing.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Mean Imputation\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "X_mean_imputed = pd.DataFrame(mean_imputer.fit_transform(X_missing), columns=numeric_cols)\n",
    "print(\"Mean Imputation - Remaining missing values:\", X_mean_imputed.isnull().sum().sum())\n",
    "\n",
    "# Method 2: Median Imputation\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "X_median_imputed = pd.DataFrame(median_imputer.fit_transform(X_missing), columns=numeric_cols)\n",
    "print(\"Median Imputation - Remaining missing values:\", X_median_imputed.isnull().sum().sum())\n",
    "\n",
    "# Method 3: KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(X_missing), columns=numeric_cols)\n",
    "print(\"KNN Imputation - Remaining missing values:\", X_knn_imputed.isnull().sum().sum())\n",
    "\n",
    "# Method 4: MICE\n",
    "mice_imputer = IterativeImputer(random_state=42, max_iter=10)\n",
    "X_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(X_missing), columns=numeric_cols)\n",
    "print(\"MICE Imputation - Remaining missing values:\", X_mice_imputed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare imputation methods - glucose variable\n",
    "glucose_original = ehr_data['glucose']\n",
    "glucose_missing_mask = ehr_missing['glucose'].isnull()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Comparison of Imputation Methods: Glucose (MCAR)', fontsize=14, fontweight='bold')\n",
    "\n",
    "axes[0, 0].hist(glucose_original, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Data')\n",
    "axes[0, 0].set_xlabel('Glucose (mg/dL)')\n",
    "\n",
    "axes[0, 1].hist(ehr_missing['glucose'].dropna(), bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 1].set_title(f'With Missing ({glucose_missing_mask.sum()} values)')\n",
    "\n",
    "axes[0, 2].hist(X_mean_imputed['glucose'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 2].set_title('Mean Imputation')\n",
    "\n",
    "axes[1, 0].hist(X_median_imputed['glucose'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 0].set_title('Median Imputation')\n",
    "\n",
    "axes[1, 1].hist(X_knn_imputed['glucose'], bins=30, alpha=0.7, color='teal', edgecolor='black')\n",
    "axes[1, 1].set_title('KNN Imputation')\n",
    "\n",
    "axes[1, 2].hist(X_mice_imputed['glucose'], bins=30, alpha=0.7, color='brown', edgecolor='black')\n",
    "axes[1, 2].set_title('MICE Imputation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"\\nGlucose Statistics Comparison:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Original', 'With Missing', 'Mean', 'Median', 'KNN', 'MICE'],\n",
    "    'Mean': [\n",
    "        glucose_original.mean(),\n",
    "        ehr_missing['glucose'].mean(),\n",
    "        X_mean_imputed['glucose'].mean(),\n",
    "        X_median_imputed['glucose'].mean(),\n",
    "        X_knn_imputed['glucose'].mean(),\n",
    "        X_mice_imputed['glucose'].mean()\n",
    "    ],\n",
    "    'Std': [\n",
    "        glucose_original.std(),\n",
    "        ehr_missing['glucose'].std(),\n",
    "        X_mean_imputed['glucose'].std(),\n",
    "        X_median_imputed['glucose'].std(),\n",
    "        X_knn_imputed['glucose'].std(),\n",
    "        X_mice_imputed['glucose'].std()\n",
    "    ]\n",
    "})\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Step 1: Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean-imputed data for PCA\n",
    "pca_data = X_mean_imputed.copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: STANDARDIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(pca_data)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=pca_data.columns)\n",
    "\n",
    "print(f\"\\nOriginal data - Mean: {pca_data.mean().mean():.2f}, Std: {pca_data.std().mean():.2f}\")\n",
    "print(f\"Scaled data - Mean: {X_scaled_df.mean().mean():.6f}, Std: {X_scaled_df.std().mean():.6f}\")\n",
    "\n",
    "# Visualize standardization effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].boxplot([pca_data.iloc[:, i] for i in range(min(10, len(pca_data.columns)))],\n",
    "                labels=[col[:8] for col in pca_data.columns[:10]])\n",
    "axes[0].set_title('Before Standardization', fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].boxplot([X_scaled_df.iloc[:, i] for i in range(min(10, len(X_scaled_df.columns)))],\n",
    "                labels=[col[:8] for col in X_scaled_df.columns[:10]])\n",
    "axes[1].set_title('After Standardization', fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA Step 2: Compute Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 2: COVARIANCE MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute covariance matrix\n",
    "cov_matrix_np = np.cov(X_scaled.T)\n",
    "\n",
    "print(f\"\\nCovariance matrix shape: {cov_matrix_np.shape}\")\n",
    "print(f\"Is symmetric? {np.allclose(cov_matrix_np, cov_matrix_np.T)}\")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cov_matrix_np, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Covariance')\n",
    "plt.title('Covariance Matrix Heatmap', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PCA Step 3: Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 3: EIGENDECOMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix_np)\n",
    "\n",
    "# Sort by eigenvalue\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = eigenvalues_sorted / eigenvalues_sorted.sum()\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(f\"\\nTop 5 eigenvalues: {eigenvalues_sorted[:5]}\")\n",
    "print(f\"Top 5 explained variance ratios: {explained_variance_ratio[:5]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, 21), eigenvalues_sorted[:20], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Eigenvalue', fontweight='bold')\n",
    "axes[0].set_title('Scree Plot', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, 21), cumulative_variance_ratio[:20] * 100, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=80, color='g', linestyle='--', label='80% variance')\n",
    "axes[1].axhline(y=95, color='b', linestyle='--', label='95% variance')\n",
    "axes[1].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance (%)', fontweight='bold')\n",
    "axes[1].set_title('Cumulative Variance Explained', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PCA Step 4: Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: PROJECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_components = 10\n",
    "W = eigenvectors_sorted[:, :n_components]\n",
    "Z_manual = X_scaled @ W\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X_scaled.shape}\")\n",
    "print(f\"Projection matrix shape: {W.shape}\")\n",
    "print(f\"Projected data shape: {Z_manual.shape}\")\n",
    "print(f\"Variance explained: {cumulative_variance_ratio[n_components-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete PCA Workflow with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE PCA WORKFLOW (scikit-learn)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "Z_sklearn = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nExplained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"\\nManual and sklearn results match? {np.allclose(np.abs(Z_manual), np.abs(Z_sklearn))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize PCA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D projection\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(Z_sklearn[:, 0], Z_sklearn[:, 1], \n",
    "                     c=ehr_data['age'], cmap='viridis', alpha=0.6, s=50, edgecolor='black')\n",
    "plt.colorbar(scatter, label='Age (years)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontweight='bold')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontweight='bold')\n",
    "plt.title('PCA: First Two Principal Components', fontweight='bold', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D projection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(Z_sklearn[:, 0], Z_sklearn[:, 1], Z_sklearn[:, 2],\n",
    "                    c=ehr_data['age'], cmap='plasma', alpha=0.6, s=50, edgecolor='black')\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontweight='bold')\n",
    "ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})', fontweight='bold')\n",
    "ax.set_title('PCA: 3D Projection', fontweight='bold', fontsize=14)\n",
    "plt.colorbar(scatter, label='Age (years)', pad=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component loadings\n",
    "feature_names = pca_data.columns[:15]\n",
    "loadings = pca.components_[:3, :15].T\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, loadings[:, 0], width, label='PC1', alpha=0.8)\n",
    "plt.bar(x, loadings[:, 1], width, label='PC2', alpha=0.8)\n",
    "plt.bar(x + width, loadings[:, 2], width, label='PC3', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Features', fontweight='bold')\n",
    "plt.ylabel('Loading', fontweight='bold')\n",
    "plt.title('PCA Component Loadings', fontweight='bold', fontsize=14)\n",
    "plt.xticks(x, [name[:10] for name in feature_names], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Healthcare Data** is heterogeneous, longitudinal, and high-dimensional with challenges in privacy, quality, and interoperability\n",
    "\n",
    "2. **Missing Data** mechanisms (MCAR, MAR, MNAR) require different handling strategies - from simple imputation to advanced methods like MICE\n",
    "\n",
    "3. **Data Artifacts** from technical and systemic sources impact model performance and require careful detection and mitigation\n",
    "\n",
    "4. **PCA Four Steps:**\n",
    "   - Step 1: Standardize features\n",
    "   - Step 2: Compute covariance matrix\n",
    "   - Step 3: Eigendecomposition\n",
    "   - Step 4: Select and project components\n",
    "\n",
    "5. **Best Practice:** Data quality determines AI success - invest time in understanding your data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
