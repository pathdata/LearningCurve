{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx2jvRGz92yX"
      },
      "source": [
        "# Introduction to AI and Machine Learning\n",
        "## Complete Assignment Solutions & Code Examples\n",
        "\n",
        "**Course Duration:** 45 minutes (3 assignments Ã— 15 minutes)\n",
        "\n",
        "This notebook contains:\n",
        "- Assignment 1: AI vs ML Classification (15 min)\n",
        "- Assignment 2: Common ML Models Comparison (15 min)\n",
        "- Assignment 3: Evaluating ML Models (15 min)\n",
        "- Bonus: Additional ML Code Examples\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzSQ2TUb92yc"
      },
      "source": [
        "## Setup: Import Required Libraries\n",
        "\n",
        "First, let's import all the libraries we'll need for the assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g0iBByO92yd"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning models\n",
        "from sklearn.datasets import load_breast_cancer, make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Supervised learning models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "# Visualization (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 100)\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkFtZqUT92yf"
      },
      "source": [
        "---\n",
        "# Assignment 1: AI vs ML Classification\n",
        "**Duration:** 15 minutes\n",
        "\n",
        "## Task\n",
        "Classify each real-world example as **AI**, **Machine Learning**, or **Deep Learning**. Provide reasoning for each classification.\n",
        "\n",
        "## Examples to Classify:\n",
        "1. Chess Engine (Stockfish)\n",
        "2. Netflix Recommendations\n",
        "3. Spam Filter\n",
        "4. Self-Driving Car\n",
        "5. Voice Assistant (Siri/Alexa)\n",
        "6. Credit Scoring System\n",
        "7. Face Recognition\n",
        "8. Chatbot with pre-programmed responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls7DwB4i92yg"
      },
      "outputs": [],
      "source": [
        "# Assignment 1 Solution\n",
        "\n",
        "classifications = {\n",
        "    \"Chess Engine (Stockfish)\": {\n",
        "        \"category\": \"AI (Traditional)\",\n",
        "        \"reasoning\": \"Uses rule-based algorithms and minimax search with alpha-beta pruning. \"\n",
        "                    \"It's AI because it exhibits intelligent behavior but doesn't learn from data.\"\n",
        "    },\n",
        "\n",
        "    \"Netflix Recommendations\": {\n",
        "        \"category\": \"Machine Learning\",\n",
        "        \"reasoning\": \"Uses collaborative filtering and content-based algorithms that learn from \"\n",
        "                    \"user viewing patterns and preferences. Classic supervised/unsupervised ML.\"\n",
        "    },\n",
        "\n",
        "    \"Spam Filter\": {\n",
        "        \"category\": \"Machine Learning\",\n",
        "        \"reasoning\": \"Learns to classify emails as spam/not spam from labeled examples using \"\n",
        "                    \"algorithms like Naive Bayes or Logistic Regression. Supervised learning.\"\n",
        "    },\n",
        "\n",
        "    \"Self-Driving Car\": {\n",
        "        \"category\": \"Deep Learning + ML + AI\",\n",
        "        \"reasoning\": \"Combines deep learning (computer vision for object detection), traditional \"\n",
        "                    \"ML (decision making), and AI (path planning, control systems).\"\n",
        "    },\n",
        "\n",
        "    \"Voice Assistant (Siri/Alexa)\": {\n",
        "        \"category\": \"Deep Learning + AI\",\n",
        "        \"reasoning\": \"Uses deep neural networks for speech recognition and natural language \"\n",
        "                    \"understanding, plus rule-based systems for task execution.\"\n",
        "    },\n",
        "\n",
        "    \"Credit Scoring System\": {\n",
        "        \"category\": \"Machine Learning\",\n",
        "        \"reasoning\": \"Predicts loan default risk using historical data. Uses regression or \"\n",
        "                    \"classification algorithms trained on features like income, credit history.\"\n",
        "    },\n",
        "\n",
        "    \"Face Recognition\": {\n",
        "        \"category\": \"Deep Learning\",\n",
        "        \"reasoning\": \"Uses convolutional neural networks (CNNs) to learn hierarchical features \"\n",
        "                    \"from face images. Requires deep architectures for high accuracy.\"\n",
        "    },\n",
        "\n",
        "    \"Chatbot with pre-programmed responses\": {\n",
        "        \"category\": \"AI (Traditional/Rule-based)\",\n",
        "        \"reasoning\": \"Follows if-then rules and pattern matching without learning from data. \"\n",
        "                    \"Traditional AI approach, not ML.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 100)\n",
        "print(\"ASSIGNMENT 1: AI vs ML Classification Results\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "results_df = pd.DataFrame([\n",
        "    {\"Example\": example, \"Category\": info[\"category\"], \"Reasoning\": info[\"reasoning\"]}\n",
        "    for example, info in classifications.items()\n",
        "])\n",
        "\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\nâœ“ Assignment 1 Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oHkf1NQ92yg"
      },
      "source": [
        "### Key Insights from Assignment 1\n",
        "\n",
        "**The Hierarchy:**\n",
        "- **AI (Artificial Intelligence)** - Broad field encompassing all intelligent behavior in machines\n",
        "- **Machine Learning** - Subset of AI that learns from data\n",
        "- **Deep Learning** - Subset of ML using neural networks with multiple layers\n",
        "\n",
        "**Remember:** Not all AI is ML, and not all ML is Deep Learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qagqlUO492yh"
      },
      "source": [
        "---\n",
        "# Assignment 2: Common ML Models Comparison\n",
        "**Duration:** 15 minutes\n",
        "\n",
        "## Task\n",
        "Train three different machine learning models on the breast cancer dataset and compare their performance.\n",
        "\n",
        "**Models to Compare:**\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. Random Forest\n",
        "\n",
        "## Questions to Answer:\n",
        "1. Which model achieved the highest accuracy?\n",
        "2. Which model is most interpretable? Why?\n",
        "3. If you had to deploy one model, which would you choose and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx8WFPYv92yh"
      },
      "source": [
        "### Step 1: Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apuAuzc292yh"
      },
      "outputs": [],
      "source": [
        "# Load the breast cancer dataset\n",
        "print(\"Loading breast cancer dataset...\\n\")\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Dataset information\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of classes: {len(set(y))}\")\n",
        "print(f\"Class names: {data.target_names}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(pd.Series(y).value_counts().to_dict())\n",
        "print(f\"  0 (malignant): {sum(y==0)}\")\n",
        "print(f\"  1 (benign): {sum(y==1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xpTcuKJ92yi"
      },
      "source": [
        "### Step 2: Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2-BNbTs92yi"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "print(f\"\\nTrain set class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "print(f\"Test set class distribution: {pd.Series(y_test).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64-bSyQ92yi"
      },
      "source": [
        "### Step 3: Train and Evaluate All Three Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFi1R74492yj"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42, max_depth=5),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"Training and Evaluating Models\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"{'='*100}\")\n",
        "\n",
        "    # Train\n",
        "    print(\"Training...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "\n",
        "    print(f\"\\n{name} Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9veIHD992yj"
      },
      "source": [
        "### Step 4: Compare Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmKfoAmn92yj"
      },
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': list(results.values())\n",
        "}).sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 100)\n",
        "print(\"\\n\", comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(comparison_df['Model'], comparison_df['Accuracy'], color=['#3b82f6', '#10b981', '#a855f7'])\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0.85, 1.0)\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (model, acc) in enumerate(zip(comparison_df['Model'], comparison_df['Accuracy'])):\n",
        "    plt.text(i, acc + 0.005, f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuljEYXd92yk"
      },
      "source": [
        "### Step 5: Answer Assignment Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE7COglM92yk"
      },
      "outputs": [],
      "source": [
        "best_model = max(results, key=results.get)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"ANSWERS TO ASSIGNMENT QUESTIONS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\n1. Best Performing Model: {best_model}\")\n",
        "print(f\"   Accuracy: {results[best_model]:.4f} ({results[best_model]*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n2. Most Interpretable Model: Decision Tree\")\n",
        "print(\"   Reasoning:\")\n",
        "print(\"   - Decision trees can be visualized as flowcharts\")\n",
        "print(\"   - Each decision path is clear and traceable\")\n",
        "print(\"   - You can see exactly which features and thresholds were used\")\n",
        "print(\"   - Non-technical stakeholders can understand the logic\")\n",
        "\n",
        "print(\"\\n3. Recommended Model for Deployment: Random Forest\")\n",
        "print(\"   Reasoning:\")\n",
        "print(\"   - Highest accuracy while maintaining reasonable interpretability\")\n",
        "print(\"   - More robust than single decision tree (ensemble reduces overfitting)\")\n",
        "print(\"   - Provides feature importance for explainability\")\n",
        "print(\"   - Good balance between performance and interpretability\")\n",
        "print(\"   - Handles non-linear relationships well\")\n",
        "print(\"   - Less prone to overfitting compared to deep decision trees\")\n",
        "\n",
        "print(\"\\nâœ“ Assignment 2 Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCR8RUvq92yk"
      },
      "source": [
        "---\n",
        "# Assignment 3: Evaluating ML Models\n",
        "**Duration:** 15 minutes\n",
        "\n",
        "## Task\n",
        "Implement comprehensive evaluation metrics and interpret the results using an **imbalanced dataset**.\n",
        "\n",
        "## Questions to Answer:\n",
        "1. Why might accuracy be misleading for this dataset?\n",
        "2. Which metric is most important for imbalanced data?\n",
        "3. Interpret the confusion matrix\n",
        "\n",
        "## Bonus Challenge:\n",
        "Compare the model's performance using:\n",
        "- Standard train-test split\n",
        "- 5-fold cross-validation\n",
        "- 10-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLpcyg2G92yk"
      },
      "source": [
        "### Step 1: Create an Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQt_F5H392yl"
      },
      "outputs": [],
      "source": [
        "# Create imbalanced dataset (90% class 0, 10% class 1)\n",
        "print(\"Creating imbalanced dataset...\\n\")\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
        "    flip_y=0.05,  # Add 5% noise\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "print(f\"\\nClass 0 (majority): {sum(y==0)} samples ({sum(y==0)/len(y)*100:.1f}%)\")\n",
        "print(f\"Class 1 (minority): {sum(y==1)} samples ({sum(y==1)/len(y)*100:.1f}%)\")\n",
        "print(f\"\\nImbalance ratio: {sum(y==0)/sum(y==1):.1f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UXcMq2992yl"
      },
      "source": [
        "### Step 2: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPn37-jj92yl"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Training Random Forest...\\n\")\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"âœ“ Model trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFd-cvO92yl"
      },
      "source": [
        "### Step 3: Calculate ALL Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3j7w3XP92yl"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"COMPREHENSIVE EVALUATION METRICS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "print(\"\\nMetric Interpretations:\")\n",
        "print(f\"- Accuracy:  Percentage of correct predictions overall\")\n",
        "print(f\"- Precision: Of predicted positives, {precision*100:.1f}% were actually positive\")\n",
        "print(f\"- Recall:    Of actual positives, we found {recall*100:.1f}%\")\n",
        "print(f\"- F1-Score:  Harmonic mean of precision and recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vd3a63Y92ym"
      },
      "source": [
        "### Step 4: Confusion Matrix Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMNO-jrj92ym"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"CONFUSION MATRIX ANALYSIS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"                  Predicted\")\n",
        "print(f\"                  Negative  Positive\")\n",
        "print(f\"Actual Negative  [{tn:5d}     {fp:5d}]\")\n",
        "print(f\"       Positive  [{fn:5d}     {tp:5d}]\")\n",
        "\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  True Negatives (TN):  {tn:3d} - Correctly predicted negative\")\n",
        "print(f\"  False Positives (FP): {fp:3d} - Incorrectly predicted positive (Type I Error)\")\n",
        "print(f\"  False Negatives (FN): {fn:3d} - Incorrectly predicted negative (Type II Error)\")\n",
        "print(f\"  True Positives (TP):  {tp:3d} - Correctly predicted positive\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0zC0Sp92ym"
      },
      "source": [
        "### Step 5: Cross-Validation Analysis (Bonus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cgLu1-g92ym"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 100)\n",
        "print(\"CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 5-Fold Cross-Validation\n",
        "print(\"\\nRunning 5-Fold Cross-Validation...\")\n",
        "cv_scores_5 = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"5-Fold CV Scores: {cv_scores_5}\")\n",
        "print(f\"Mean: {cv_scores_5.mean():.4f} (+/- {cv_scores_5.std() * 2:.4f})\")\n",
        "\n",
        "# 10-Fold Cross-Validation\n",
        "print(\"\\nRunning 10-Fold Cross-Validation...\")\n",
        "cv_scores_10 = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
        "print(f\"10-Fold CV Scores: {cv_scores_10}\")\n",
        "print(f\"Mean: {cv_scores_10.mean():.4f} (+/- {cv_scores_10.std() * 2:.4f})\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"-\" * 100)\n",
        "comparison_data = {\n",
        "    'Strategy': ['Train-Test Split', '5-Fold CV', '10-Fold CV'],\n",
        "    'Accuracy': [accuracy, cv_scores_5.mean(), cv_scores_10.mean()],\n",
        "    'Std Dev': ['N/A', f'{cv_scores_5.std():.4f}', f'{cv_scores_10.std():.4f}']\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nComparison of Evaluation Strategies:\")\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccTnzLmD92ym"
      },
      "source": [
        "### Step 6: Answer Assignment Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0O7QBL792ym"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"ANSWERS TO ASSIGNMENT QUESTIONS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "baseline_accuracy = sum(y_test==0)/len(y_test)\n",
        "\n",
        "print(\"\\n1. Why might accuracy be misleading for this dataset?\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   Current model accuracy: {accuracy:.4f}\")\n",
        "print(f\"   Baseline (always predict majority class): {baseline_accuracy:.4f}\")\n",
        "print()\n",
        "print(\"   Answer:\")\n",
        "print(\"   With 90% of samples in class 0, a naive classifier that always predicts\")\n",
        "print(\"   class 0 would achieve ~90% accuracy without learning anything useful!\")\n",
        "print(\"   Accuracy doesn't tell us how well we detect the minority class (class 1),\")\n",
        "print(\"   which is often the class we care about most (e.g., fraud, disease).\")\n",
        "\n",
        "print(\"\\n2. Which metric is most important for imbalanced data?\")\n",
        "print(\"-\" * 80)\n",
        "print(\"   Answer: F1-Score or Precision-Recall metrics\")\n",
        "print()\n",
        "print(\"   Reasoning:\")\n",
        "print(f\"   - Precision ({precision:.4f}): Of predicted positives, how many are correct?\")\n",
        "print(f\"   - Recall ({recall:.4f}): Of actual positives, how many did we find?\")\n",
        "print(f\"   - F1-Score ({f1:.4f}): Harmonic mean balancing both\")\n",
        "print()\n",
        "print(\"   For imbalanced data, these metrics focus on the minority class.\")\n",
        "print(\"   The choice depends on the cost of errors:\")\n",
        "print(\"   - High precision needed: Minimize false alarms (e.g., spam detection)\")\n",
        "print(\"   - High recall needed: Don't miss positives (e.g., disease diagnosis)\")\n",
        "\n",
        "print(\"\\n3. Interpret the confusion matrix:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   True Negatives (TN={tn}): Correctly identified {tn} negative cases\")\n",
        "print(f\"   False Positives (FP={fp}): Incorrectly flagged {fp} negative cases as positive\")\n",
        "print(f\"   False Negatives (FN={fn}): Missed {fn} actual positive cases\")\n",
        "print(f\"   True Positives (TP={tp}): Correctly identified {tp} positive cases\")\n",
        "print()\n",
        "print(f\"   Model Performance:\")\n",
        "print(f\"   - Correctly classified {tn + tp}/{len(y_test)} samples ({accuracy:.1%})\")\n",
        "print(f\"   - Of {tn+fp} predicted negatives, {tn/(tn+fp):.1%} were correct\")\n",
        "print(f\"   - Of {tp+fn} actual positives, found {tp/(tp+fn):.1%} (recall)\")\n",
        "if fn > 0:\n",
        "    print(f\"   - âš ï¸  WARNING: Missed {fn} positive cases - may need model tuning!\")\n",
        "\n",
        "print(\"\\nâœ“ Assignment 3 Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3IQ0Bj92yn"
      },
      "source": [
        "---\n",
        "# BONUS: Additional ML Code Examples\n",
        "\n",
        "Extra techniques for advanced learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FswbpbYW92yn"
      },
      "source": [
        "## Bonus 1: Hyperparameter Tuning with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ogKh5KJ92yn"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 100)\n",
        "print(\"BONUS 1: HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "print(\"\\nSearching for best hyperparameters...\")\n",
        "print(f\"Testing {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split'])} combinations\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best F1-score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"\\nTest set performance with best model:\")\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_best):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITSM6I7d92yn"
      },
      "source": [
        "## Bonus 2: Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrpocxwC92yn"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"BONUS 2: FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Get feature importance from best model\n",
        "importances = best_model.feature_importances_\n",
        "\n",
        "# Create dataframe\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': [f'feature_{i}' for i in range(X.shape[1])],\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance_df.head(10).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(10), feature_importance_df.head(10)['importance'], color='#3b82f6')\n",
        "plt.yticks(range(10), feature_importance_df.head(10)['feature'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Top 10 Feature Importances', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW1lrrHz92yn"
      },
      "source": [
        "## Bonus 3: ROC Curve and AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Uuz5Ga92yo"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"BONUS 3: ROC CURVE AND AUC SCORE\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Get probability predictions\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation:\")\n",
        "if auc_score > 0.9:\n",
        "    print(\"âœ“ Excellent discrimination (AUC > 0.9)\")\n",
        "elif auc_score > 0.8:\n",
        "    print(\"âœ“ Good discrimination (AUC > 0.8)\")\n",
        "elif auc_score > 0.7:\n",
        "    print(\"âš  Acceptable discrimination (AUC > 0.7)\")\n",
        "else:\n",
        "    print(\"âš  Poor discrimination (AUC < 0.7)\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='#3b82f6', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJYi1mMI92yo"
      },
      "source": [
        "## Bonus 4: Learning Curve Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrBq9Vzv92yo"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"BONUS 4: LEARNING CURVE ANALYSIS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(\"\\nGenerating learning curves...\")\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    best_model,\n",
        "    X, y,\n",
        "    cv=5,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate mean and std\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "print(f\"\\nFinal training score: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n",
        "print(f\"Final validation score: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")\n",
        "\n",
        "gap = train_mean[-1] - val_mean[-1]\n",
        "print(f\"\\nTraining-Validation Gap: {gap:.4f}\")\n",
        "if gap < 0.05:\n",
        "    print(\"âœ“ Model is well-fitted (small gap between train and validation)\")\n",
        "elif gap < 0.10:\n",
        "    print(\"âš  Model may be slightly overfitting\")\n",
        "else:\n",
        "    print(\"âš  Model is overfitting (large gap between train and validation)\")\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_mean, 'o-', color='#3b82f6', label='Training score')\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='#3b82f6')\n",
        "plt.plot(train_sizes, val_mean, 'o-', color='#10b981', label='Validation score')\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='#10b981')\n",
        "plt.xlabel('Training Set Size', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Learning Curves', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfqMWDW92yt"
      },
      "source": [
        "---\n",
        "# ðŸŽ‰ Congratulations!\n",
        "\n",
        "You've completed all three assignments plus bonus examples!\n",
        "\n",
        "## Key Skills You've Mastered:\n",
        "\n",
        "âœ… AI vs ML classification and reasoning  \n",
        "âœ… Training and comparing multiple ML models  \n",
        "âœ… Comprehensive model evaluation with multiple metrics  \n",
        "âœ… Understanding imbalanced data challenges  \n",
        "âœ… Cross-validation for robust evaluation  \n",
        "âœ… Hyperparameter tuning with Grid Search  \n",
        "âœ… Feature importance analysis  \n",
        "âœ… ROC-AUC analysis  \n",
        "âœ… Learning curve interpretation  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Learning! ðŸš€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}