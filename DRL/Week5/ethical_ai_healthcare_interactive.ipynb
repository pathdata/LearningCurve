{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMTvuJ-L1UCX"
      },
      "source": [
        "# ETHICAL AI IN HEALTHCARE: Adversarial Robustness with Comprehensive Governance\n",
        "\n",
        "**Author:** Dr. Priya Lakshmi Narayanan  \n",
        "**Institution:** Institute of Cancer Research  \n",
        "**Course:** Advanced Topics in AI for Healthcare\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives:\n",
        "1. Understand technical vulnerabilities in medical AI systems\n",
        "2. Apply ethical frameworks to AI development and deployment\n",
        "3. Implement data governance and privacy-preserving techniques\n",
        "4. Evaluate clinical translation requirements and stakeholder needs\n",
        "5. Develop holistic understanding of responsible AI in healthcare\n",
        "\n",
        "---\n",
        "\n",
        "## This module demonstrates:\n",
        "- **ETHICAL CONSIDERATIONS** in AI for healthcare\n",
        "- **DATA GOVERNANCE** principles and best practices\n",
        "- **TRANSLATIONAL RESEARCH** perspectives for clinical deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmyxheXZ1UCb"
      },
      "source": [
        "## Setup: Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oJ6keL31UCc",
        "outputId": "cd60bf12-e591-4675-8fcf-cb63c52aefa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries imported successfully\n",
            "TensorFlow version: 2.19.0\n",
            "NumPy version: 2.0.2\n",
            "Pandas version: 2.2.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import json\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCUu27P11UCd"
      },
      "source": [
        "## MODULE 1: ETHICAL FRAMEWORK AND PRINCIPLES\n",
        "\n",
        "Based on:\n",
        "- Beauchamp & Childress's Four Principles (Biomedical Ethics)\n",
        "- WHO Guidelines on Ethics and Governance of AI for Health\n",
        "- IEEE P7000 Standards for Ethical AI Design\n",
        "- UK NHS AI Ethics Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67DTbL-r1UCd",
        "outputId": "5da25490-c605-41b5-c182-e379a13dd916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Ethical Framework class defined\n"
          ]
        }
      ],
      "source": [
        "class EthicalFramework:\n",
        "    \"\"\"\n",
        "    Comprehensive ethical framework for AI in healthcare.\n",
        "    \"\"\"\n",
        "\n",
        "    PRINCIPLES = {\n",
        "        'beneficence': 'Maximize benefits and minimize harm to patients',\n",
        "        'non_maleficence': 'Do no harm - ensure safety and robustness',\n",
        "        'autonomy': 'Respect patient and clinician decision-making',\n",
        "        'justice': 'Ensure fairness and equitable access',\n",
        "        'transparency': 'Make AI systems explainable and auditable',\n",
        "        'privacy': 'Protect patient confidentiality and data rights',\n",
        "        'accountability': 'Establish clear lines of responsibility'\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ethical_checklist = []\n",
        "        self.risk_assessments = []\n",
        "\n",
        "    def log_ethical_consideration(self, principle, action, rationale):\n",
        "        \"\"\"Log ethical decisions made during development\"\"\"\n",
        "        self.ethical_checklist.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'principle': principle,\n",
        "            'action': action,\n",
        "            'rationale': rationale\n",
        "        })\n",
        "        print(f\"   [ETHICS] {principle.upper()}: {action}\")\n",
        "\n",
        "    def assess_dual_use_risk(self, technology_description):\n",
        "        \"\"\"\n",
        "        Assess potential for misuse (dual-use concern).\n",
        "\n",
        "        Teaching Note: Adversarial attack techniques could be misused to:\n",
        "        - Manipulate medical records\n",
        "        - Evade disease detection systems\n",
        "        - Create fraudulent insurance claims\n",
        "\n",
        "        Mitigation: Responsible disclosure, defensive focus, educational framing\n",
        "        \"\"\"\n",
        "        risk_level = 'MODERATE'  # For educational adversarial research\n",
        "\n",
        "        self.risk_assessments.append({\n",
        "            'technology': technology_description,\n",
        "            'risk_level': risk_level,\n",
        "            'mitigation': 'Educational context only; emphasis on defense mechanisms',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        print(f\"\\n   [DUAL-USE ASSESSMENT]\")\n",
        "        print(f\"   Technology: {technology_description}\")\n",
        "        print(f\"   Risk Level: {risk_level}\")\n",
        "        print(f\"   Ethical Consideration: Teaching adversarial attacks requires careful framing\")\n",
        "        print(f\"   Mitigation: Focus on defensive applications and clinical safety\")\n",
        "\n",
        "        return risk_level\n",
        "\n",
        "    def generate_ethics_report(self):\n",
        "        \"\"\"Generate comprehensive ethics documentation\"\"\"\n",
        "        report = {\n",
        "            'ethical_framework': self.PRINCIPLES,\n",
        "            'decisions_log': self.ethical_checklist,\n",
        "            'risk_assessments': self.risk_assessments,\n",
        "            'generated_at': datetime.now().isoformat()\n",
        "        }\n",
        "        return report\n",
        "\n",
        "print(\"✓ Ethical Framework class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKtDsjWR1UCe"
      },
      "source": [
        "## MODULE 2: DATA GOVERNANCE AND PRIVACY\n",
        "\n",
        "Implements:\n",
        "- HIPAA Privacy Rule compliance\n",
        "- GDPR Article 9 (Special Category Data)\n",
        "- De-identification standards (HIPAA Safe Harbor)\n",
        "- Data provenance and lineage tracking\n",
        "- Access control and audit logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqgJ-ngh1UCf",
        "outputId": "6d112cea-48d3-4401-edd9-530a2ac39e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data Governance Framework class defined\n"
          ]
        }
      ],
      "source": [
        "class DataGovernanceFramework:\n",
        "    \"\"\"\n",
        "    Comprehensive data governance for healthcare AI.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data_lineage = []\n",
        "        self.access_log = []\n",
        "        self.consent_records = {}\n",
        "        self.deidentification_methods = []\n",
        "\n",
        "    def verify_deidentification(self, dataframe, identifier_columns):\n",
        "        \"\"\"\n",
        "        Verify proper de-identification of patient data.\n",
        "\n",
        "        HIPAA Safe Harbor Method requires removal of 18 identifiers:\n",
        "        - Names, geographic subdivisions, dates, contact information, etc.\n",
        "\n",
        "        Teaching Note: Real clinical deployment requires IRB approval and\n",
        "        proper consent for secondary use of data.\n",
        "        \"\"\"\n",
        "        print(\"\\n   [DATA GOVERNANCE] Verifying De-identification\")\n",
        "\n",
        "        phi_identifiers = {\n",
        "            'direct': ['name', 'patient_name', 'ssn', 'medical_record_number', 'patient_id'],\n",
        "            'quasi': ['date_of_birth', 'admission_date', 'zip_code', 'age_over_89'],\n",
        "            'contact': ['phone', 'email', 'address', 'ip_address']\n",
        "        }\n",
        "\n",
        "        found_identifiers = []\n",
        "        for category, identifiers in phi_identifiers.items():\n",
        "            for col in dataframe.columns:\n",
        "                if any(identifier.lower() in col.lower() for identifier in identifiers):\n",
        "                    found_identifiers.append((category, col))\n",
        "\n",
        "        if found_identifiers:\n",
        "            print(f\"   ⚠ WARNING: Potential identifiers found: {found_identifiers}\")\n",
        "            print(f\"   Action Required: Review and remove/pseudonymize these fields\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"   ✓ No direct identifiers detected in column names\")\n",
        "            print(f\"   Note: Statistical disclosure risk still requires assessment\")\n",
        "            return True\n",
        "\n",
        "    def log_data_access(self, user, action, dataset_name, purpose):\n",
        "        \"\"\"Maintain audit trail for regulatory compliance\"\"\"\n",
        "        access_record = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'user': user,\n",
        "            'action': action,\n",
        "            'dataset': dataset_name,\n",
        "            'purpose': purpose,\n",
        "            'ip_hash': hashlib.sha256(b'educational_environment').hexdigest()[:16]\n",
        "        }\n",
        "        self.access_log.append(access_record)\n",
        "\n",
        "    def track_data_provenance(self, dataset_name, source, transformations):\n",
        "        \"\"\"\n",
        "        Track data lineage for reproducibility and accountability.\n",
        "\n",
        "        Critical for:\n",
        "        - Regulatory audits (FDA, EMA)\n",
        "        - Scientific reproducibility\n",
        "        - Error tracing and correction\n",
        "        \"\"\"\n",
        "        provenance_record = {\n",
        "            'dataset': dataset_name,\n",
        "            'source': source,\n",
        "            'transformations': transformations,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'version': hashlib.sha256(str(transformations).encode()).hexdigest()[:16]\n",
        "        }\n",
        "        self.data_lineage.append(provenance_record)\n",
        "        print(f\"   [PROVENANCE] Tracked: {dataset_name} (version: {provenance_record['version']}))\")\n",
        "\n",
        "    def assess_privacy_risk(self, dataset, k_anonymity_threshold=5):\n",
        "        \"\"\"\n",
        "        Assess re-identification risk using k-anonymity concept.\n",
        "        \"\"\"\n",
        "        print(f\"\\n   [PRIVACY ASSESSMENT]\")\n",
        "        print(f\"   K-anonymity threshold: {k_anonymity_threshold}\")\n",
        "        print(f\"   Assessment: Educational synthetic/aggregated data - Low re-identification risk\")\n",
        "        print(f\"   Clinical deployment: Requires formal privacy impact assessment (PIA)\")\n",
        "\n",
        "        return {'risk_level': 'LOW', 'reason': 'Synthetic/aggregated educational data'}\n",
        "\n",
        "    def document_consent_basis(self, data_source, legal_basis):\n",
        "        \"\"\"\n",
        "        Document legal basis for data processing.\n",
        "\n",
        "        GDPR requires explicit legal basis:\n",
        "        - Consent (Article 6.1.a)\n",
        "        - Research in public interest (Article 9.2.j)\n",
        "        - Clinical care necessity (Article 9.2.h)\n",
        "        \"\"\"\n",
        "        self.consent_records[data_source] = {\n",
        "            'legal_basis': legal_basis,\n",
        "            'documented_at': datetime.now().isoformat()\n",
        "        }\n",
        "        print(f\"   [CONSENT] {data_source}: {legal_basis}\")\n",
        "\n",
        "print(\"✓ Data Governance Framework class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWY3z8Ew1UCg"
      },
      "source": [
        "## MODULE 3: CLINICAL TRANSLATION FRAMEWORK\n",
        "\n",
        "Bridge research to clinical practice with safety and efficacy focus.\n",
        "\n",
        "Based on:\n",
        "- FDA Software as Medical Device (SaMD) guidance\n",
        "- TRIPOD Statement (Transparent Reporting of Prediction Models)\n",
        "- STARD Guidelines (Diagnostic Accuracy Studies)\n",
        "- CONSORT-AI Extension (Clinical Trials for AI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmEemJYn1UCg",
        "outputId": "e1f0aa2f-bc72-4eb0-a45d-bad984212bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Clinical Translation Framework class defined\n"
          ]
        }
      ],
      "source": [
        "class ClinicalTranslationFramework:\n",
        "    \"\"\"\n",
        "    Bridge research to clinical practice with safety and efficacy focus.\n",
        "    \"\"\"\n",
        "\n",
        "    TRANSLATION_STAGES = {\n",
        "        'T0': 'Basic research and discovery',\n",
        "        'T1': 'Proof of concept in controlled settings',\n",
        "        'T2': 'Clinical validation and efficacy studies',\n",
        "        'T3': 'Implementation and dissemination',\n",
        "        'T4': 'Population health impact assessment'\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.validation_results = {}\n",
        "        self.stakeholder_requirements = {}\n",
        "        self.deployment_checklist = []\n",
        "\n",
        "    def assess_translation_stage(self, current_stage='T1'):\n",
        "        \"\"\"Identify current translation stage and requirements\"\"\"\n",
        "        print(f\"\\n   [TRANSLATION] Current Stage: {current_stage}\")\n",
        "        print(f\"   Description: {self.TRANSLATION_STAGES[current_stage]}\")\n",
        "\n",
        "        if current_stage == 'T1':\n",
        "            print(\"\\n   Requirements for T1→T2 progression:\")\n",
        "            print(\"   • Validation on independent clinical cohort\")\n",
        "            print(\"   • Prospective study design (avoid retrospective bias)\")\n",
        "            print(\"   • Clinical expert evaluation of predictions\")\n",
        "            print(\"   • Safety monitoring protocol\")\n",
        "            print(\"   • Regulatory pathway identification (FDA 510(k), De Novo, PMA)\")\n",
        "\n",
        "    def evaluate_clinical_utility(self, model, test_data, test_labels, clinical_threshold):\n",
        "        \"\"\"\n",
        "        Assess clinical utility beyond statistical metrics.\n",
        "\n",
        "        Teaching Note: High AUC ≠ Clinical Utility\n",
        "        - Consider clinical workflow integration\n",
        "        - Assess impact on patient outcomes\n",
        "        - Evaluate cost-effectiveness\n",
        "        - Measure clinician trust and adoption\n",
        "        \"\"\"\n",
        "        from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "        predictions = (model.predict(test_data).flatten() > clinical_threshold).astype(int)\n",
        "        cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "        if cm.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "            # Clinical metrics\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "\n",
        "            print(f\"\\n   [CLINICAL UTILITY ASSESSMENT]\")\n",
        "            print(f\"   Sensitivity (Recall):    {sensitivity:.3f} - Critical for disease screening\")\n",
        "            print(f\"   Specificity:             {specificity:.3f} - Reduces false alarms\")\n",
        "            print(f\"   PPV (Precision):         {ppv:.3f} - Confidence in positive prediction\")\n",
        "            print(f\"   NPV:                     {npv:.3f} - Confidence in negative prediction\")\n",
        "            print(f\"\\n   Clinical Interpretation:\")\n",
        "\n",
        "            if sensitivity < 0.85:\n",
        "                print(f\"   ⚠ Low sensitivity - May miss true cases (false negatives)\")\n",
        "                print(f\"     Risk: Delayed diagnosis, adverse patient outcomes\")\n",
        "\n",
        "            if specificity < 0.85:\n",
        "                print(f\"   ⚠ Low specificity - High false positive rate\")\n",
        "                print(f\"     Risk: Unnecessary treatments, patient anxiety, resource waste\")\n",
        "\n",
        "            if ppv < 0.7:\n",
        "                print(f\"   ⚠ Low PPV - Many positive predictions are false\")\n",
        "                print(f\"     Impact: Clinician trust, unnecessary follow-up procedures\")\n",
        "\n",
        "            return {\n",
        "                'sensitivity': sensitivity,\n",
        "                'specificity': specificity,\n",
        "                'ppv': ppv,\n",
        "                'npv': npv,\n",
        "                'clinical_threshold': clinical_threshold\n",
        "            }\n",
        "\n",
        "    def assess_deployment_readiness(self, model_performance, robustness_metrics):\n",
        "        \"\"\"Evaluate readiness for clinical deployment.\"\"\"\n",
        "        print(f\"\\n   [DEPLOYMENT READINESS ASSESSMENT]\")\n",
        "        print(f\"   {'='*60}\")\n",
        "\n",
        "        checklist = {\n",
        "            'Statistical Performance': model_performance.get('auc', 0) > 0.80,\n",
        "            'Adversarial Robustness': robustness_metrics.get('defended_accuracy', 0) > 0.75,\n",
        "            'Clinical Validation': False,  # Requires prospective study\n",
        "            'Regulatory Pathway': False,   # Requires submission\n",
        "            'Safety Monitoring': False,    # Requires infrastructure\n",
        "            'Stakeholder Engagement': False  # Requires clinician feedback\n",
        "        }\n",
        "\n",
        "        for criterion, status in checklist.items():\n",
        "            symbol = '✓' if status else '✗'\n",
        "            print(f\"   {symbol} {criterion:<30s} {'COMPLETE' if status else 'REQUIRED'}\")\n",
        "\n",
        "        readiness_score = sum(checklist.values()) / len(checklist) * 100\n",
        "        print(f\"\\n   Overall Readiness: {readiness_score:.0f}%\")\n",
        "\n",
        "        if readiness_score < 100:\n",
        "            print(f\"   ⚠ NOT READY for clinical deployment\")\n",
        "            print(f\"   Status: Research prototype - Educational/investigational use only\")\n",
        "\n",
        "        return checklist\n",
        "\n",
        "    def engage_stakeholders(self, stakeholder_group, requirements):\n",
        "        \"\"\"Document stakeholder requirements and concerns.\"\"\"\n",
        "        self.stakeholder_requirements[stakeholder_group] = requirements\n",
        "        print(f\"\\n   [STAKEHOLDER: {stakeholder_group}]\")\n",
        "        for req in requirements:\n",
        "            print(f\"   • {req}\")\n",
        "\n",
        "print(\"✓ Clinical Translation Framework class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr8rsSnD1UCh"
      },
      "source": [
        "## MODULE 4: FAIRNESS AND BIAS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3kibkbZ1UCh",
        "outputId": "dbce2265-c990-4bae-b92f-657b58149817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fairness Auditor class defined\n"
          ]
        }
      ],
      "source": [
        "class FairnessAuditor:\n",
        "    \"\"\"\n",
        "    Evaluate model fairness across protected demographic groups.\n",
        "\n",
        "    Fairness Metrics:\n",
        "    - Demographic Parity: Equal positive prediction rates\n",
        "    - Equalized Odds: Equal TPR and FPR across groups\n",
        "    - Predictive Parity: Equal PPV across groups\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fairness_results = {}\n",
        "\n",
        "    def evaluate_demographic_parity(self, predictions, sensitive_attribute, attribute_name):\n",
        "        \"\"\"Check if positive prediction rate is equal across groups.\"\"\"\n",
        "        unique_groups = np.unique(sensitive_attribute)\n",
        "        positive_rates = {}\n",
        "\n",
        "        print(f\"\\n   [FAIRNESS AUDIT] Demographic Parity - {attribute_name}\")\n",
        "\n",
        "        for group in unique_groups:\n",
        "            group_mask = sensitive_attribute == group\n",
        "            group_predictions = predictions[group_mask]\n",
        "            positive_rate = np.mean(group_predictions)\n",
        "            positive_rates[group] = positive_rate\n",
        "            print(f\"   Group {group}: {positive_rate:.3f} positive prediction rate\")\n",
        "\n",
        "        # Calculate disparity\n",
        "        rates = list(positive_rates.values())\n",
        "        max_disparity = max(rates) - min(rates)\n",
        "\n",
        "        if max_disparity > 0.10:  # 10% threshold\n",
        "            print(f\"   ⚠ DISPARITY DETECTED: {max_disparity:.3f} difference\")\n",
        "            print(f\"   Action: Investigate potential bias in training data or features\")\n",
        "        else:\n",
        "            print(f\"   ✓ Acceptable demographic parity (disparity: {max_disparity:.3f})\")\n",
        "\n",
        "        return positive_rates, max_disparity\n",
        "\n",
        "    def evaluate_equalized_odds(self, y_true, y_pred, sensitive_attribute, attribute_name):\n",
        "        \"\"\"Check if TPR and FPR are equal across groups.\"\"\"\n",
        "        unique_groups = np.unique(sensitive_attribute)\n",
        "\n",
        "        print(f\"\\n   [FAIRNESS AUDIT] Equalized Odds - {attribute_name}\")\n",
        "\n",
        "        group_metrics = {}\n",
        "        for group in unique_groups:\n",
        "            group_mask = sensitive_attribute == group\n",
        "            y_true_group = y_true[group_mask]\n",
        "            y_pred_group = y_pred[group_mask]\n",
        "\n",
        "            if len(y_true_group) > 0:\n",
        "                cm = confusion_matrix(y_true_group, y_pred_group)\n",
        "                if cm.shape == (2, 2):\n",
        "                    tn, fp, fn, tp = cm.ravel()\n",
        "                    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "                    group_metrics[group] = {'TPR': tpr, 'FPR': fpr}\n",
        "                    print(f\"   Group {group}: TPR={tpr:.3f}, FPR={fpr:.3f}\")\n",
        "\n",
        "        # Check for disparities\n",
        "        if len(group_metrics) >= 2:\n",
        "            tprs = [m['TPR'] for m in group_metrics.values()]\n",
        "            fprs = [m['FPR'] for m in group_metrics.values()]\n",
        "\n",
        "            tpr_disparity = max(tprs) - min(tprs)\n",
        "            fpr_disparity = max(fprs) - min(fprs)\n",
        "\n",
        "            print(f\"\\n   TPR Disparity: {tpr_disparity:.3f}\")\n",
        "            print(f\"   FPR Disparity: {fpr_disparity:.3f}\")\n",
        "\n",
        "            if tpr_disparity > 0.10 or fpr_disparity > 0.10:\n",
        "                print(f\"   ⚠ Fairness concern - Consider bias mitigation strategies\")\n",
        "            else:\n",
        "                print(f\"   ✓ Acceptable equalized odds\")\n",
        "\n",
        "        return group_metrics\n",
        "\n",
        "print(\"✓ Fairness Auditor class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orG6d_J1UCi"
      },
      "source": [
        "## MODULE 5: ADVERSARIAL ROBUSTNESS WITH ETHICAL CONTEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fse6TjGj1UCi",
        "outputId": "a86b83b1-1923-42ce-d7dd-adfa0f814372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Fixed adversarial attack function defined\n",
            "✓ Model building and adversarial attack functions defined\n"
          ]
        }
      ],
      "source": [
        "def build_robust_model(input_dim, name=\"healthcare_model\"):\n",
        "    \"\"\"\n",
        "    Build neural network with robustness considerations.\n",
        "\n",
        "    Ethical Design Choices:\n",
        "    - Dropout for uncertainty quantification\n",
        "    - L2 regularization to prevent overfitting to spurious correlations\n",
        "    - Calibrated outputs for confident predictions only\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(64, activation='relu', input_shape=(input_dim,),\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dropout(0.3),  # Uncertainty estimation\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dense(32, activation='relu',\n",
        "                          kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(16, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')  # Calibrated probabilities\n",
        "    ], name=name)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def fgsm_attack_with_safety_bounds(model, x, y, epsilon, feature_bounds=None):\n",
        "    \"\"\"\n",
        "    Fast Gradient Sign Method with clinical safety constraints.\n",
        "\n",
        "    Ethical Consideration: Perturbations must remain clinically plausible\n",
        "    - Blood pressure cannot be negative\n",
        "    - Age cannot decrease\n",
        "    - Categorical variables have discrete values\n",
        "    \"\"\"\n",
        "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "    # FIX: Reshape y_tensor to match model output shape (batch_size, 1)\n",
        "    y_tensor = tf.reshape(y_tensor, (-1, 1))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x_tensor)\n",
        "        prediction = model(x_tensor)\n",
        "        loss = keras.losses.binary_crossentropy(y_tensor, prediction)\n",
        "\n",
        "    gradient = tape.gradient(loss, x_tensor)\n",
        "    signed_grad = tf.sign(gradient)\n",
        "\n",
        "    # Apply perturbation\n",
        "    adversarial = x_tensor + epsilon * signed_grad\n",
        "\n",
        "    # Apply safety bounds if provided (ensure clinical plausibility)\n",
        "    if feature_bounds is not None:\n",
        "        for i, (min_val, max_val) in enumerate(feature_bounds):\n",
        "            adversarial[:, i] = tf.clip_by_value(adversarial[:, i], min_val, max_val)\n",
        "\n",
        "    return adversarial.numpy()\n",
        "\n",
        "print(\"✓ Fixed adversarial attack function defined\")\n",
        "print(\"✓ Model building and adversarial attack functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyRwwRyD1UCi"
      },
      "source": [
        "## DATA LOADING: Choose Your Data Source\n",
        "\n",
        "**Option 1:** Synthetic educational data (default - no files required)  \n",
        "**Option 2:** Load from `coad_msi_mutation_details.csv`\n",
        "\n",
        "### Configure your data source below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSjugu1b1UCj",
        "outputId": "00f6554e-c2ed-4963-d567-25d3565434e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source configured: synthetic\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# USER CONFIGURATION: Choose your data source\n",
        "# ============================================================================\n",
        "\n",
        "# Set DATA_SOURCE to either 'synthetic' or 'csv'\n",
        "DATA_SOURCE = 'synthetic'  # Change to 'csv' to use your CSV file\n",
        "\n",
        "# If using CSV, specify the file path\n",
        "CSV_FILE_PATH = 'coad_msi_mutation_details.csv'\n",
        "\n",
        "print(f\"Data source configured: {DATA_SOURCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ldOTr731UCj",
        "outputId": "d3a6b8a8-8c3a-45f3-c9b3-4c58f5aa334c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generate synthetic medical data for educational purposes.\n",
        "\n",
        "    This creates a realistic dataset without using real patient data,\n",
        "    ensuring complete privacy protection.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LOADING SYNTHETIC EDUCATIONAL DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_synthetic, y_synthetic = make_classification(\n",
        "        n_samples=500,\n",
        "        n_features=8,\n",
        "        n_informative=6,\n",
        "        n_classes=2,\n",
        "        weights=[0.7, 0.3],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    feature_names = ['clinical_feature_' + str(i) for i in range(8)]\n",
        "    df_synthetic = pd.DataFrame(X_synthetic, columns=feature_names)\n",
        "    df_synthetic['outcome'] = y_synthetic\n",
        "\n",
        "    print(f\"\\n   ✓ Generated synthetic dataset\")\n",
        "    print(f\"   • Samples: {len(df_synthetic)}\")\n",
        "    print(f\"   • Features: {len(feature_names)}\")\n",
        "    print(f\"   • Outcome distribution: {dict(df_synthetic['outcome'].value_counts())}\")\n",
        "    print(f\"   • Data type: Synthetic (no real patient data)\")\n",
        "\n",
        "    return df_synthetic, feature_names, 'Synthetic Educational Data'\n",
        "\n",
        "\n",
        "def load_coad_csv_data(csv_path):\n",
        "    \"\"\"\n",
        "    Load COAD MSI mutation data from CSV file.\n",
        "\n",
        "    This function loads real research data and performs necessary\n",
        "    preprocessing while maintaining data governance standards.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LOADING COAD MSI MUTATION DATA FROM CSV\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"CSV file not found: {csv_path}\\n\"\n",
        "            f\"Please ensure 'coad_msi_mutation_details.csv' is in the current directory\\n\"\n",
        "            f\"or change DATA_SOURCE to 'synthetic' to use synthetic data instead.\"\n",
        "        )\n",
        "\n",
        "    # Load CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"\\n   ✓ Loaded CSV file: {csv_path}\")\n",
        "    print(f\"   • Total rows: {len(df)}\")\n",
        "    print(f\"   • Total columns: {len(df.columns)}\")\n",
        "\n",
        "    # Display column names\n",
        "    print(f\"\\n   Available columns:\")\n",
        "    for col in df.columns:\n",
        "        print(f\"   • {col}\")\n",
        "\n",
        "    # Create binary outcome from mutation_status\n",
        "    if 'mutation_status' in df.columns:\n",
        "        # MSI = 1 (positive), MSS = 0 (negative)\n",
        "        df['outcome'] = (df['mutation_status'] == 'MSI').astype(int)\n",
        "        print(f\"\\n   ✓ Created binary outcome from mutation_status\")\n",
        "        print(f\"   • MSI (positive): {sum(df['outcome'] == 1)}\")\n",
        "        print(f\"   • MSS (negative): {sum(df['outcome'] == 0)}\")\n",
        "    elif 'outcome' in df.columns:\n",
        "        # Outcome already exists\n",
        "        print(f\"\\n   ✓ Using existing 'outcome' column\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"CSV file must contain either 'mutation_status' or 'outcome' column\\n\"\n",
        "            \"Available columns: \" + \", \".join(df.columns)\n",
        "        )\n",
        "\n",
        "    # Select numeric features for modeling\n",
        "    # Prioritize: age, num_msi_mutations, survival_time\n",
        "    potential_features = ['age', 'num_msi_mutations', 'survival_time']\n",
        "\n",
        "    # Add any other numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numeric_cols = [col for col in numeric_cols if col not in ['outcome'] + potential_features]\n",
        "\n",
        "    # Combine features\n",
        "    feature_cols = [col for col in potential_features if col in df.columns] + numeric_cols[:5]\n",
        "\n",
        "    # Handle missing values\n",
        "    df_clean = df[feature_cols + ['outcome']].copy()\n",
        "\n",
        "    # Fill missing numeric values with median\n",
        "    for col in feature_cols:\n",
        "        if df_clean[col].dtype in [np.float64, np.int64]:\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # Remove rows with missing outcome\n",
        "    df_clean = df_clean.dropna(subset=['outcome'])\n",
        "\n",
        "    print(f\"\\n   ✓ Preprocessed data\")\n",
        "    print(f\"   • Selected features: {feature_cols}\")\n",
        "    print(f\"   • Final sample size: {len(df_clean)}\")\n",
        "    print(f\"   • Outcome distribution: {dict(df_clean['outcome'].value_counts())}\")\n",
        "\n",
        "    return df_clean, feature_cols, 'TCGA COAD MSI Mutation Data'\n",
        "\n",
        "\n",
        "def load_data(data_source='synthetic', csv_path=None):\n",
        "    \"\"\"\n",
        "    Universal data loading function.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data_source : str\n",
        "        Either 'synthetic' or 'csv'\n",
        "    csv_path : str\n",
        "        Path to CSV file (required if data_source='csv')\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df : DataFrame\n",
        "        Preprocessed data with features and outcome\n",
        "    feature_names : list\n",
        "        List of feature column names\n",
        "    dataset_name : str\n",
        "        Descriptive name of the dataset\n",
        "    \"\"\"\n",
        "    if data_source.lower() == 'synthetic':\n",
        "        return load_synthetic_data()\n",
        "    elif data_source.lower() == 'csv':\n",
        "        if csv_path is None:\n",
        "            raise ValueError(\"csv_path must be provided when data_source='csv'\")\n",
        "        return load_coad_csv_data(csv_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid data_source: {data_source}. Must be 'synthetic' or 'csv'\")\n",
        "\n",
        "print(\"✓ Data loading functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgvEf35F1UCj"
      },
      "source": [
        "### Load the selected dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEQboKuL1UCk",
        "outputId": "9df7877f-3ae4-4f7d-9d34-7ddfb6facafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING SYNTHETIC EDUCATIONAL DATA\n",
            "================================================================================\n",
            "\n",
            "   ✓ Generated synthetic dataset\n",
            "   • Samples: 500\n",
            "   • Features: 8\n",
            "   • Outcome distribution: {0: np.int64(348), 1: np.int64(152)}\n",
            "   • Data type: Synthetic (no real patient data)\n",
            "\n",
            "================================================================================\n",
            "✓ Successfully loaded: Synthetic Educational Data\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load data based on configuration\n",
        "try:\n",
        "    df_data, feature_names, dataset_name = load_data(\n",
        "        data_source=DATA_SOURCE,\n",
        "        csv_path=CSV_FILE_PATH if DATA_SOURCE == 'csv' else None\n",
        "    )\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"✓ Successfully loaded: {dataset_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠ ERROR loading data: {e}\")\n",
        "    print(f\"\\nTip: If you want to use synthetic data, set DATA_SOURCE = 'synthetic' above\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtpFQIo1UCk"
      },
      "source": [
        "## COMPREHENSIVE ETHICAL AI WORKFLOW\n",
        "\n",
        "Now we'll run the complete analysis integrating:\n",
        "1. Ethical Framework\n",
        "2. Data Governance\n",
        "3. Clinical Translation\n",
        "4. Model Development\n",
        "5. Fairness Evaluation\n",
        "6. Adversarial Robustness Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn_IzJiJ1UCk"
      },
      "source": [
        "### STEP 1: Establish Ethical Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "z9gyy-Ln1UCk",
        "outputId": "02410e9b-e476-4cf5-e567-72474c1e93dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: ESTABLISH ETHICAL FRAMEWORK\n",
            "================================================================================\n",
            "   [ETHICS] BENEFICENCE: Developing adversarial robustness techniques\n",
            "   [ETHICS] TRANSPARENCY: Open-source educational materials with full documentation\n",
            "\n",
            "   [DUAL-USE ASSESSMENT]\n",
            "   Technology: Adversarial attack techniques for medical AI systems\n",
            "   Risk Level: MODERATE\n",
            "   Ethical Consideration: Teaching adversarial attacks requires careful framing\n",
            "   Mitigation: Focus on defensive applications and clinical safety\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MODERATE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: ESTABLISH ETHICAL FRAMEWORK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "ethics = EthicalFramework()\n",
        "governance = DataGovernanceFramework()\n",
        "translation = ClinicalTranslationFramework()\n",
        "fairness = FairnessAuditor()\n",
        "\n",
        "# Document ethical basis for research\n",
        "ethics.log_ethical_consideration(\n",
        "    'beneficence',\n",
        "    'Developing adversarial robustness techniques',\n",
        "    'Improves safety and reliability of medical AI systems, protecting patients from prediction errors'\n",
        ")\n",
        "\n",
        "ethics.log_ethical_consideration(\n",
        "    'transparency',\n",
        "    'Open-source educational materials with full documentation',\n",
        "    'Enables peer review and promotes responsible AI literacy in healthcare community'\n",
        ")\n",
        "\n",
        "# Assess dual-use risk\n",
        "ethics.assess_dual_use_risk(\n",
        "    'Adversarial attack techniques for medical AI systems'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A23x2B_t1UCk"
      },
      "source": [
        "### STEP 2: Data Governance and Privacy Protection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY7xiuvY1UCk",
        "outputId": "9484f00c-6d7f-4cc6-88ff-8885f15e8ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: DATA GOVERNANCE AND PRIVACY PROTECTION\n",
            "================================================================================\n",
            "   [CONSENT] Synthetic Educational Dataset: Synthetic data generated for educational purposes - No patient data\n",
            "   [PROVENANCE] Tracked: Synthetic Educational Data (version: 8d49e00fdf70eb13))\n",
            "\n",
            "   [DATA GOVERNANCE] Verifying De-identification\n",
            "   ✓ No direct identifiers detected in column names\n",
            "   Note: Statistical disclosure risk still requires assessment\n",
            "\n",
            "   [PRIVACY ASSESSMENT]\n",
            "   K-anonymity threshold: 5\n",
            "   Assessment: Educational synthetic/aggregated data - Low re-identification risk\n",
            "   Clinical deployment: Requires formal privacy impact assessment (PIA)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'risk_level': 'LOW', 'reason': 'Synthetic/aggregated educational data'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: DATA GOVERNANCE AND PRIVACY PROTECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Document consent basis\n",
        "if DATA_SOURCE == 'csv':\n",
        "    governance.document_consent_basis(\n",
        "        'TCGA Colorectal Cancer Dataset',\n",
        "        'Public research resource - GDPR Article 9.2.j (Research in public interest)'\n",
        "    )\n",
        "else:\n",
        "    governance.document_consent_basis(\n",
        "        'Synthetic Educational Dataset',\n",
        "        'Synthetic data generated for educational purposes - No patient data'\n",
        "    )\n",
        "\n",
        "# Log data access\n",
        "governance.log_data_access(\n",
        "    user='instructor@university.edu',\n",
        "    action='Model Training',\n",
        "    dataset_name=dataset_name,\n",
        "    purpose='Teaching adversarial robustness concepts'\n",
        ")\n",
        "\n",
        "# Track data provenance\n",
        "if DATA_SOURCE == 'csv':\n",
        "    governance.track_data_provenance(\n",
        "        dataset_name=dataset_name,\n",
        "        source=CSV_FILE_PATH,\n",
        "        transformations=['Missing value imputation', 'Feature selection', 'Train-test split', 'Standardization']\n",
        "    )\n",
        "else:\n",
        "    governance.track_data_provenance(\n",
        "        dataset_name=dataset_name,\n",
        "        source='sklearn.make_classification',\n",
        "        transformations=['Synthetic generation', 'Train-test split', 'Standardization']\n",
        "    )\n",
        "\n",
        "# Verify de-identification\n",
        "governance.verify_deidentification(df_data, identifier_columns=[])\n",
        "\n",
        "# Assess privacy risk\n",
        "governance.assess_privacy_risk(df_data, k_anonymity_threshold=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRgN2X3S1UCl"
      },
      "source": [
        "### STEP 3: Clinical Translation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxgu4EP41UCl",
        "outputId": "6a3faae5-f330-4520-d539-495f83793cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: CLINICAL TRANSLATION FRAMEWORK\n",
            "================================================================================\n",
            "\n",
            "   [TRANSLATION] Current Stage: T1\n",
            "   Description: Proof of concept in controlled settings\n",
            "\n",
            "   Requirements for T1→T2 progression:\n",
            "   • Validation on independent clinical cohort\n",
            "   • Prospective study design (avoid retrospective bias)\n",
            "   • Clinical expert evaluation of predictions\n",
            "   • Safety monitoring protocol\n",
            "   • Regulatory pathway identification (FDA 510(k), De Novo, PMA)\n",
            "\n",
            "   [STAKEHOLDER: Clinicians]\n",
            "   • Model predictions must include confidence intervals\n",
            "   • Explanations must be clinically interpretable\n",
            "   • Integration with existing EHR workflow required\n",
            "   • Liability and clinical oversight framework needed\n",
            "\n",
            "   [STAKEHOLDER: Patients]\n",
            "   • Right to know when AI is used in care decisions\n",
            "   • Right to opt-out or request human review\n",
            "   • Protection against discriminatory algorithmic bias\n",
            "   • Privacy and data security guarantees\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: CLINICAL TRANSLATION FRAMEWORK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Assess current translation stage\n",
        "translation.assess_translation_stage('T1')\n",
        "\n",
        "# Engage stakeholders\n",
        "translation.engage_stakeholders(\n",
        "    'Clinicians',\n",
        "    [\n",
        "        'Model predictions must include confidence intervals',\n",
        "        'Explanations must be clinically interpretable',\n",
        "        'Integration with existing EHR workflow required',\n",
        "        'Liability and clinical oversight framework needed'\n",
        "    ]\n",
        ")\n",
        "\n",
        "translation.engage_stakeholders(\n",
        "    'Patients',\n",
        "    [\n",
        "        'Right to know when AI is used in care decisions',\n",
        "        'Right to opt-out or request human review',\n",
        "        'Protection against discriminatory algorithmic bias',\n",
        "        'Privacy and data security guarantees'\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUDZqiQz1UCl"
      },
      "source": [
        "### STEP 4: Model Development with Ethical Safeguards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEVj3hqh1UCl",
        "outputId": "53926428-1b1a-49a9-f9a3-9fab84b12a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: MODEL DEVELOPMENT WITH ROBUSTNESS\n",
            "================================================================================\n",
            "\n",
            "   Data split:\n",
            "   • Training samples: 400\n",
            "   • Test samples: 100\n",
            "   • Number of features: 8\n",
            "\n",
            "   Building model with ethical design principles...\n",
            "\n",
            "   Training model...\n",
            "\n",
            "   Model Performance:\n",
            "   • Test Accuracy: 0.9300\n",
            "   • Test AUC: 0.9369\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: MODEL DEVELOPMENT WITH ROBUSTNESS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare data\n",
        "X = df_data[feature_names].values\n",
        "y = df_data['outcome'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\n   Data split:\")\n",
        "print(f\"   • Training samples: {len(X_train)}\")\n",
        "print(f\"   • Test samples: {len(X_test)}\")\n",
        "print(f\"   • Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "# Build model with ethical design\n",
        "print(\"\\n   Building model with ethical design principles...\")\n",
        "model = build_robust_model(X_train.shape[1], name=\"ethical_healthcare_model\")\n",
        "\n",
        "# Train with early stopping (avoid overfitting)\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(\"\\n   Training model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate performance\n",
        "loss, accuracy, auc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\n   Model Performance:\")\n",
        "print(f\"   • Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"   • Test AUC: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv1wd_5U1UCl"
      },
      "source": [
        "### STEP 5: Clinical Utility Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7_MpkMi1UCl",
        "outputId": "8a42834b-9608-40fe-8e5e-f06249f74ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: CLINICAL UTILITY ASSESSMENT\n",
            "================================================================================\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "   [CLINICAL UTILITY ASSESSMENT]\n",
            "   Sensitivity (Recall):    0.833 - Critical for disease screening\n",
            "   Specificity:             0.971 - Reduces false alarms\n",
            "   PPV (Precision):         0.926 - Confidence in positive prediction\n",
            "   NPV:                     0.932 - Confidence in negative prediction\n",
            "\n",
            "   Clinical Interpretation:\n",
            "   ⚠ Low sensitivity - May miss true cases (false negatives)\n",
            "     Risk: Delayed diagnosis, adverse patient outcomes\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: CLINICAL UTILITY ASSESSMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "clinical_metrics = translation.evaluate_clinical_utility(\n",
        "    model, X_test, y_test, clinical_threshold=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHegUfve1UCm"
      },
      "source": [
        "### STEP 6: Fairness and Bias Audit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp-cz8c_1UCm",
        "outputId": "9754c0aa-2b96-4207-a316-66be61755df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: FAIRNESS AND BIAS AUDIT\n",
            "================================================================================\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\n",
            "   [FAIRNESS AUDIT] Demographic Parity - Demographic Group\n",
            "   Group 0: 0.286 positive prediction rate\n",
            "   Group 1: 0.243 positive prediction rate\n",
            "   ✓ Acceptable demographic parity (disparity: 0.042)\n",
            "\n",
            "   [FAIRNESS AUDIT] Equalized Odds - Demographic Group\n",
            "   Group 0: TPR=0.810, FPR=0.024\n",
            "   Group 1: TPR=0.889, FPR=0.036\n",
            "\n",
            "   TPR Disparity: 0.079\n",
            "   FPR Disparity: 0.012\n",
            "   ✓ Acceptable equalized odds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.int64(0): {'TPR': np.float64(0.8095238095238095),\n",
              "  'FPR': np.float64(0.023809523809523808)},\n",
              " np.int64(1): {'TPR': np.float64(0.8888888888888888),\n",
              "  'FPR': np.float64(0.03571428571428571)}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: FAIRNESS AND BIAS AUDIT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Simulate protected attribute (e.g., demographic group)\n",
        "# In real deployment, would use actual demographic data\n",
        "simulated_demographic = np.random.choice([0, 1], size=len(X_test), p=[0.6, 0.4])\n",
        "\n",
        "predictions = (model.predict(X_test).flatten() > 0.5).astype(int)\n",
        "\n",
        "fairness.evaluate_demographic_parity(\n",
        "    predictions, simulated_demographic, 'Demographic Group'\n",
        ")\n",
        "\n",
        "fairness.evaluate_equalized_odds(\n",
        "    y_test, predictions, simulated_demographic, 'Demographic Group'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiYVaEVW1UCm"
      },
      "source": [
        "### STEP 7: Adversarial Robustness Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOrw1WPW1UCm",
        "outputId": "8d595285-7cf7-46b6-cc90-c04adb5559c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: ADVERSARIAL ROBUSTNESS EVALUATION\n",
            "================================================================================\n",
            "\n",
            "   [SAFETY CONSIDERATION]\n",
            "   Testing robustness to adversarial perturbations is critical because:\n",
            "   • Malicious actors could manipulate inputs to evade detection\n",
            "   • Measurement errors or data corruption could occur in practice\n",
            "   • Model must be robust to small input variations for clinical safety\n",
            "\n",
            "   Adversarial Attack Results (ε=0.1):\n",
            "   • Clean Accuracy:       0.9300\n",
            "   • Adversarial Accuracy: 0.8700\n",
            "   • Accuracy Drop:        0.0600 (6.5%)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: ADVERSARIAL ROBUSTNESS EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n   [SAFETY CONSIDERATION]\")\n",
        "print(\"   Testing robustness to adversarial perturbations is critical because:\")\n",
        "print(\"   • Malicious actors could manipulate inputs to evade detection\")\n",
        "print(\"   • Measurement errors or data corruption could occur in practice\")\n",
        "print(\"   • Model must be robust to small input variations for clinical safety\")\n",
        "\n",
        "# Test adversarial robustness\n",
        "epsilon = 0.1\n",
        "X_adversarial = fgsm_attack_with_safety_bounds(\n",
        "    model, X_test, y_test, epsilon=epsilon\n",
        ")\n",
        "\n",
        "# Evaluate on adversarial examples\n",
        "loss_adv, acc_adv, auc_adv = model.evaluate(X_adversarial, y_test, verbose=0)\n",
        "\n",
        "print(f\"\\n   Adversarial Attack Results (ε={epsilon}):\")\n",
        "print(f\"   • Clean Accuracy:       {accuracy:.4f}\")\n",
        "print(f\"   • Adversarial Accuracy: {acc_adv:.4f}\")\n",
        "print(f\"   • Accuracy Drop:        {(accuracy - acc_adv):.4f} ({(accuracy - acc_adv)/accuracy * 100:.1f}%)\")\n",
        "\n",
        "if (accuracy - acc_adv) > 0.15:\n",
        "    print(f\"\\n   ⚠ SAFETY CONCERN: Model shows high vulnerability to adversarial perturbations\")\n",
        "    print(f\"   Recommendation: Implement adversarial training or input validation before deployment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-98B73a1UCm"
      },
      "source": [
        "### STEP 8: Deployment Readiness Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbdd2bnR1UCm",
        "outputId": "2b94c04b-5fdc-43ce-9bc9-dcd98ad63a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: DEPLOYMENT READINESS EVALUATION\n",
            "================================================================================\n",
            "\n",
            "   [DEPLOYMENT READINESS ASSESSMENT]\n",
            "   ============================================================\n",
            "   ✓ Statistical Performance        COMPLETE\n",
            "   ✓ Adversarial Robustness         COMPLETE\n",
            "   ✗ Clinical Validation            REQUIRED\n",
            "   ✗ Regulatory Pathway             REQUIRED\n",
            "   ✗ Safety Monitoring              REQUIRED\n",
            "   ✗ Stakeholder Engagement         REQUIRED\n",
            "\n",
            "   Overall Readiness: 33%\n",
            "   ⚠ NOT READY for clinical deployment\n",
            "   Status: Research prototype - Educational/investigational use only\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: DEPLOYMENT READINESS EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_performance = {'auc': auc, 'accuracy': accuracy}\n",
        "robustness_metrics = {'defended_accuracy': acc_adv}\n",
        "\n",
        "readiness = translation.assess_deployment_readiness(\n",
        "    model_performance, robustness_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXTNa3wl1UCn"
      },
      "source": [
        "### STEP 9: Documentation and Reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCH7tNYs1UCn",
        "outputId": "bfedda07-012f-4f5d-867b-2f2b85666bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 9: DOCUMENTATION AND REPORTING\n",
            "================================================================================\n",
            "\n",
            "   Generating comprehensive documentation...\n",
            "   ✓ Model Card generated: ethical_ai_model_card.json\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: DOCUMENTATION AND REPORTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n   Generating comprehensive documentation...\")\n",
        "\n",
        "# Ethics report\n",
        "ethics_report = ethics.generate_ethics_report()\n",
        "\n",
        "# Model card (following Model Cards for Model Reporting framework)\n",
        "model_card = {\n",
        "    'model_details': {\n",
        "        'name': 'Ethical Healthcare AI Classifier',\n",
        "        'version': '1.0-educational',\n",
        "        'date': datetime.now().isoformat(),\n",
        "        'type': 'Binary classification neural network',\n",
        "        'intended_use': 'Educational demonstration only - NOT for clinical use',\n",
        "        'dataset': dataset_name\n",
        "    },\n",
        "    'performance': {\n",
        "        'accuracy': float(accuracy),\n",
        "        'auc': float(auc),\n",
        "        'clinical_metrics': clinical_metrics\n",
        "    },\n",
        "    'fairness': {\n",
        "        'evaluated': True,\n",
        "        'demographic_parity': 'Assessed',\n",
        "        'equalized_odds': 'Assessed'\n",
        "    },\n",
        "    'robustness': {\n",
        "        'adversarial_tested': True,\n",
        "        'epsilon': epsilon,\n",
        "        'adversarial_accuracy': float(acc_adv)\n",
        "    },\n",
        "    'limitations': [\n",
        "        f'Trained on {dataset_name}',\n",
        "        'Not validated on real clinical population' if DATA_SOURCE == 'synthetic' else 'Requires validation on independent cohort',\n",
        "        'No prospective clinical validation',\n",
        "        f'Moderate vulnerability to adversarial perturbations ({(accuracy - acc_adv)/accuracy * 100:.1f}% drop)',\n",
        "        'Requires regulatory approval before clinical deployment'\n",
        "    ],\n",
        "    'ethical_considerations': ethics_report,\n",
        "    'data_governance': {\n",
        "        'provenance': governance.data_lineage,\n",
        "        'access_log': governance.access_log\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save model card\n",
        "with open('ethical_ai_model_card.json', 'w') as f:\n",
        "    json.dump(model_card, f, indent=2, default=str)\n",
        "\n",
        "print(\"   ✓ Model Card generated: ethical_ai_model_card.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXo91oy61UCn"
      },
      "source": [
        "## FINAL SUMMARY AND KEY TAKEAWAYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbFMQQOu1UCn",
        "outputId": "3ce5f585-5095-4a1f-ccdf-89419bd57967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LEARNING OUTCOMES - SUMMARY\n",
            "================================================================================\n",
            "\n",
            "1. ETHICAL FRAMEWORK APPLIED:\n",
            "   • Documented 2 ethical decisions\n",
            "   • Assessed dual-use risks and mitigation strategies\n",
            "   • Applied principles of beneficence, transparency, and safety\n",
            "\n",
            "2. DATA GOVERNANCE IMPLEMENTED:\n",
            "   • Verified de-identification compliance\n",
            "   • Tracked data provenance and lineage\n",
            "   • Documented legal basis for data processing (GDPR)\n",
            "   • Maintained audit trail (1 access records)\n",
            "\n",
            "3. CLINICAL TRANSLATION EVALUATED:\n",
            "   • Assessed translation stage (T1 - Proof of concept)\n",
            "   • Evaluated clinical utility beyond statistical metrics\n",
            "   • Engaged stakeholders (clinicians, patients)\n",
            "   • Identified deployment requirements and gaps\n",
            "\n",
            "4. FAIRNESS AND EQUITY ASSESSED:\n",
            "   • Evaluated demographic parity across groups\n",
            "   • Assessed equalized odds (TPR/FPR equity)\n",
            "   • Identified potential sources of algorithmic bias\n",
            "\n",
            "5. ROBUSTNESS AND SAFETY TESTED:\n",
            "   • Evaluated adversarial robustness (accuracy drop: 6.00%)\n",
            "   • Identified vulnerability to perturbations\n",
            "   • Recommended mitigation strategies for deployment\n",
            "\n",
            "================================================================================\n",
            "KEY TAKEAWAYS FOR RESPONSIBLE AI IN HEALTHCARE\n",
            "================================================================================\n",
            "\n",
            "1. TECHNICAL EXCELLENCE IS NECESSARY BUT NOT SUFFICIENT\n",
            "   High accuracy alone does not ensure patient safety or clinical utility\n",
            "\n",
            "2. ETHICAL FRAMEWORKS MUST BE INTEGRATED FROM THE START\n",
            "   Ethics cannot be an afterthought - embed principles in design\n",
            "\n",
            "3. DATA GOVERNANCE IS CRITICAL FOR TRUST AND COMPLIANCE\n",
            "   Proper data handling, privacy protection, and documentation are essential\n",
            "\n",
            "4. CLINICAL TRANSLATION REQUIRES MULTIDISCIPLINARY COLLABORATION\n",
            "   Engage clinicians, patients, and regulators throughout development\n",
            "\n",
            "5. FAIRNESS AND EQUITY DEMAND CONTINUOUS VIGILANCE\n",
            "   Bias can emerge at any stage - requires ongoing monitoring and mitigation\n",
            "\n",
            "6. ROBUSTNESS TESTING IS A SAFETY IMPERATIVE\n",
            "   Healthcare AI must be robust to adversarial attacks and data perturbations\n",
            "\n",
            "7. TRANSPARENCY AND DOCUMENTATION ENABLE ACCOUNTABILITY\n",
            "   Comprehensive documentation supports auditing, debugging, and trust\n",
            "\n",
            "8. KNOW YOUR LIMITATIONS AND COMMUNICATE THEM CLEARLY\n",
            "   Be honest about what the model can and cannot do\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EXERCISE COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Dataset used: Synthetic Educational Data\n",
            "Model accuracy: 0.9300\n",
            "Adversarial robustness: 0.8700\n",
            "\n",
            "Generated files:\n",
            "  • ethical_ai_model_card.json\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LEARNING OUTCOMES - SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. ETHICAL FRAMEWORK APPLIED:\")\n",
        "print(f\"   • Documented {len(ethics.ethical_checklist)} ethical decisions\")\n",
        "print(f\"   • Assessed dual-use risks and mitigation strategies\")\n",
        "print(f\"   • Applied principles of beneficence, transparency, and safety\")\n",
        "\n",
        "print(\"\\n2. DATA GOVERNANCE IMPLEMENTED:\")\n",
        "print(f\"   • Verified de-identification compliance\")\n",
        "print(f\"   • Tracked data provenance and lineage\")\n",
        "print(f\"   • Documented legal basis for data processing (GDPR)\")\n",
        "print(f\"   • Maintained audit trail ({len(governance.access_log)} access records)\")\n",
        "\n",
        "print(\"\\n3. CLINICAL TRANSLATION EVALUATED:\")\n",
        "print(f\"   • Assessed translation stage (T1 - Proof of concept)\")\n",
        "print(f\"   • Evaluated clinical utility beyond statistical metrics\")\n",
        "print(f\"   • Engaged stakeholders (clinicians, patients)\")\n",
        "print(f\"   • Identified deployment requirements and gaps\")\n",
        "\n",
        "print(\"\\n4. FAIRNESS AND EQUITY ASSESSED:\")\n",
        "print(f\"   • Evaluated demographic parity across groups\")\n",
        "print(f\"   • Assessed equalized odds (TPR/FPR equity)\")\n",
        "print(f\"   • Identified potential sources of algorithmic bias\")\n",
        "\n",
        "print(\"\\n5. ROBUSTNESS AND SAFETY TESTED:\")\n",
        "print(f\"   • Evaluated adversarial robustness (accuracy drop: {(accuracy - acc_adv):.2%})\")\n",
        "print(f\"   • Identified vulnerability to perturbations\")\n",
        "print(f\"   • Recommended mitigation strategies for deployment\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY TAKEAWAYS FOR RESPONSIBLE AI IN HEALTHCARE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. TECHNICAL EXCELLENCE IS NECESSARY BUT NOT SUFFICIENT\n",
        "   High accuracy alone does not ensure patient safety or clinical utility\n",
        "\n",
        "2. ETHICAL FRAMEWORKS MUST BE INTEGRATED FROM THE START\n",
        "   Ethics cannot be an afterthought - embed principles in design\n",
        "\n",
        "3. DATA GOVERNANCE IS CRITICAL FOR TRUST AND COMPLIANCE\n",
        "   Proper data handling, privacy protection, and documentation are essential\n",
        "\n",
        "4. CLINICAL TRANSLATION REQUIRES MULTIDISCIPLINARY COLLABORATION\n",
        "   Engage clinicians, patients, and regulators throughout development\n",
        "\n",
        "5. FAIRNESS AND EQUITY DEMAND CONTINUOUS VIGILANCE\n",
        "   Bias can emerge at any stage - requires ongoing monitoring and mitigation\n",
        "\n",
        "6. ROBUSTNESS TESTING IS A SAFETY IMPERATIVE\n",
        "   Healthcare AI must be robust to adversarial attacks and data perturbations\n",
        "\n",
        "7. TRANSPARENCY AND DOCUMENTATION ENABLE ACCOUNTABILITY\n",
        "   Comprehensive documentation supports auditing, debugging, and trust\n",
        "\n",
        "8. KNOW YOUR LIMITATIONS AND COMMUNICATE THEM CLEARLY\n",
        "   Be honest about what the model can and cannot do\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXERCISE COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset used: {dataset_name}\")\n",
        "print(f\"Model accuracy: {accuracy:.4f}\")\n",
        "print(f\"Adversarial robustness: {acc_adv:.4f}\")\n",
        "print(f\"\\nGenerated files:\")\n",
        "print(f\"  • ethical_ai_model_card.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4Yek_d1UCs"
      },
      "source": [
        "## Discussion Questions for Students\n",
        "\n",
        "1. **Ethical Trade-offs:** What are the potential consequences if this model were deployed clinically without the ethical and governance frameworks demonstrated here?\n",
        "\n",
        "2. **Stakeholder Perspectives:** How might different stakeholders (patients, clinicians, administrators) prioritize the various ethical principles differently?\n",
        "\n",
        "3. **Clinical Validation:** What additional validation studies would be required before clinical deployment?\n",
        "\n",
        "4. **Transparency vs. Performance:** How can we balance model transparency (explainability) with performance?\n",
        "\n",
        "5. **Fairness Mechanisms:** What mechanisms could detect and respond to fairness violations post-deployment?\n",
        "\n",
        "6. **Liability Assignment:** How should liability be assigned if an AI system makes a harmful prediction?\n",
        "\n",
        "7. **Security Implications:** What are the implications of adversarial vulnerability for medical AI security?\n",
        "\n",
        "8. **Continuous Improvement:** How can we ensure continuous monitoring and improvement after deployment?\n",
        "\n",
        "---\n",
        "\n",
        "### End of Notebook\n",
        "\n",
        "**Author:** Dr. Priya Lakshmi Narayanan  \n",
        "**Contact:** priya.narayanan@icr.ac.uk  \n",
        "**License:** Educational use only"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}